{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://www.iranketab.ir/book'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' response = requests.get(url=main_url , headers=headers)\\nprint(response.status_code) '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" response = requests.get(url=main_url , headers=headers)\n",
    "print(response.status_code) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" soup = BeautifulSoup(response.content, 'html.parser') \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" soup = BeautifulSoup(response.content, 'html.parser') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' page_number = 1  # Start with the first page\\n\\nrequest_count = 0\\n\\nsession = requests.Session()\\nvisited_urls = set()\\n\\nwith open(\\'BooksUrls.csv\\', \\'w\\', newline=\\'\\') as csvfile:\\n    url_writer = csv.writer(csvfile)\\n\\n    while True:\\n        current_page_url = f\"{main_url}?pagenumber={page_number}&pagesize=20\"\\n        response = session.get(current_page_url)\\n\\n        if response.status_code != 200:\\n            break\\n\\n        soup = BeautifulSoup(response.text, \\'html.parser\\')\\n        allBooks = soup.find_all(\"a\", class_=\"product-item-link\")\\n        \\n        for book_url in allBooks:\\n            href = book_url.get(\"href\")\\n            if href:\\n                full_url = f\\'{main_url}{href}\\'\\n                if full_url not in visited_urls:\\n                    visited_urls.add(full_url)\\n                    url_writer.writerow([full_url])\\n                    request_count += 1\\n\\n                    if request_count % 500 == 0:\\n                        time.sleep(1)\\n\\n        next_page_link = soup.find(\"a\", {\"data-page-no\": str(page_number + 1)})\\n\\n        if next_page_link:\\n            page_number += 1\\n        else:\\n            break '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" page_number = 1  # Start with the first page\n",
    "\n",
    "request_count = 0\n",
    "\n",
    "session = requests.Session()\n",
    "visited_urls = set()\n",
    "\n",
    "with open('BooksUrls.csv', 'w', newline='') as csvfile:\n",
    "    url_writer = csv.writer(csvfile)\n",
    "\n",
    "    while True:\n",
    "        current_page_url = f\"{main_url}?pagenumber={page_number}&pagesize=20\"\n",
    "        response = session.get(current_page_url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        allBooks = soup.find_all(\"a\", class_=\"product-item-link\")\n",
    "        \n",
    "        for book_url in allBooks:\n",
    "            href = book_url.get(\"href\")\n",
    "            if href:\n",
    "                full_url = f'{main_url}{href}'\n",
    "                if full_url not in visited_urls:\n",
    "                    visited_urls.add(full_url)\n",
    "                    url_writer.writerow([full_url])\n",
    "                    request_count += 1\n",
    "\n",
    "                    if request_count % 500 == 0:\n",
    "                        time.sleep(1)\n",
    "\n",
    "        next_page_link = soup.find(\"a\", {\"data-page-no\": str(page_number + 1)})\n",
    "\n",
    "        if next_page_link:\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_file = 'final_links.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BookURLs = pd.read_csv(\\'BooksUrls.csv\\', header=None)\\nurls = BookURLs[0].tolist()\\nurls = [url.replace(\"/book\", \"\", 1) for url in urls]\\nlink = pd.DataFrame({\\'final_books_Url\\': urls}) '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" BookURLs = pd.read_csv('BooksUrls.csv', header=None)\n",
    "urls = BookURLs[0].tolist()\n",
    "urls = [url.replace(\"/book\", \"\", 1) for url in urls]\n",
    "link = pd.DataFrame({'final_books_Url': urls}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link.to_csv('final_books_Url.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BookURLs = pd.read_csv('final_books_Url.csv', header=None)\n",
    "links = BookURLs[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for link: https://www.iranketab.ir/book/87210-\n",
      "Exceeded 30 redirects.\n",
      "Request failed for link: https://www.iranketab.ir/book/90319-\n",
      "Exceeded 30 redirects.\n",
      "Request failed for link: https://www.iranketab.ir/book/90591-\n",
      "Exceeded 30 redirects.\n",
      "Request failed for link: https://www.iranketab.ir/book/120654-\n",
      "Exceeded 30 redirects.\n"
     ]
    }
   ],
   "source": [
    "retry_strategy = Retry(\n",
    "    total=100,\n",
    "    backoff_factor=2,\n",
    "    status_forcelist=[500, 502, 503, 504],\n",
    ")\n",
    "\n",
    "session = requests.Session()\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "csv_file = 'scraped_data5.csv'\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.DictWriter(file, fieldnames=[\n",
    "        'book_id', 'print_series', 'size', 'type_of_print', 'translator',\n",
    "        'shabak', 'gregorian_publish_year', 'solar_publish_year', 'page_count',\n",
    "        'Persian Title', 'English Title', 'Off', 'Rate', 'Break Price', 'Special_Price',\n",
    "        'Exist', 'Publisher', 'publisher id', 'Writer', 'writer id', 'Description', 'Feature', 'Category'\n",
    "    ])\n",
    "\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    session = requests.Session()\n",
    "    request_count = 0\n",
    "\n",
    "    for link in links[50000:]:\n",
    "        try:\n",
    "            response = session.get(link)\n",
    "            if response.status_code == 200:\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                clearfix_elements = soup.find_all('div', class_='clearfix')\n",
    "                writer_id = None\n",
    "                publisher_id = None\n",
    "                for clearfix in clearfix_elements:\n",
    "                    product_name_element = clearfix.find(class_='product-name')\n",
    "                    if product_name_element:\n",
    "                        persian_title = product_name_element.text.strip()\n",
    "\n",
    "                        en_name_element = clearfix.find('div', class_='product-name-englishname ltr')\n",
    "                        if en_name_element:\n",
    "                            english_title = en_name_element.text.strip()\n",
    "                        else:\n",
    "                            english_title = \"\"\n",
    "\n",
    "                        off = clearfix.find('div', style='float: left;font-size: 12px;line-height: 1.375;background-color: #fb3449;color: #fff;padding: 5px 30px 3px;-webkit-border-radius: 0 16px 16px 16px;border-radius: 0 16px 16px 16px;')\n",
    "                        if off:\n",
    "                            off_percent = off.text.strip()\n",
    "                        else:\n",
    "                            off_percent = 0\n",
    "                        rating_div = clearfix.find('div', class_='my-rating')\n",
    "                        if rating_div:\n",
    "                            data_rating = rating_div.get('data-rating')\n",
    "                        else:\n",
    "                            data_rating = 0\n",
    "                        break_price = clearfix.find('span' , class_= 'price price-broken')\n",
    "                        if break_price:\n",
    "                            before_price = break_price.text.strip()\n",
    "                        else :\n",
    "                            before_price = np.nan\n",
    "                        special_price = clearfix.find('span' , class_= 'price price-special')\n",
    "                        if special_price:\n",
    "                            after_price = special_price.text.strip()\n",
    "                        else :\n",
    "                            after_price = np.nan\n",
    "                        exists_book_element = clearfix.find('li', class_='exists-book')\n",
    "                        if exists_book_element:\n",
    "                            exists_book_text = exists_book_element.text.strip()\n",
    "                        else:\n",
    "                            exists_book_text = 'ناموجود'\n",
    "                        publisher_element = clearfix.find('div', class_='col-xs-12 prodoct-attribute-items')\n",
    "                        if publisher_element:\n",
    "                            publisher_span = publisher_element.find('span', class_='prodoct-attribute-item')\n",
    "                            if publisher_span and publisher_span.text.strip() == 'انتشارات:':\n",
    "                                publisher_text = publisher_element.find('span', class_='prodoct-attribute-item').find_next('span').text.strip()\n",
    "                            else:\n",
    "                                publisher_text = \"\"\n",
    "                        else:\n",
    "                            publisher_text = \"\"\n",
    "                        #########################################################################################################################################\n",
    "                        look_for_id = clearfix.find('div', class_='row clearfix')\n",
    "                        for id_publisher_and_writer in look_for_id.find_all('a'):\n",
    "                            if id_publisher_and_writer:\n",
    "                                href = id_publisher_and_writer.get('href')\n",
    "                                if 'publisher' in href:\n",
    "                                    publisher_id = int(href.split('/publisher/')[1].split('-')[0])\n",
    "                                elif 'profile' in href:\n",
    "                                    writer_id = int(href.split('/profile/')[1].split('-')[0])\n",
    "                        #########################################################################################################################################\n",
    "                        writer_element = clearfix.find('span', itemprop='name')\n",
    "                        if writer_element:\n",
    "                            writer_text = writer_element.text.strip()\n",
    "                        else:\n",
    "                            writer_text = \"\"\n",
    "\n",
    "                        description_element = soup.find('div', class_='product-description')\n",
    "                        if description_element:\n",
    "                            description_text = description_element.text.strip()\n",
    "                        else:\n",
    "                            description_text = \"\"\n",
    "                        feature_elements = soup.find_all('div', class_='product-features')\n",
    "                        features = []\n",
    "                        for feature_element in feature_elements:\n",
    "                            h4_elements = feature_element.find_all('h4')\n",
    "                            for h4_element in h4_elements:\n",
    "                                feature_text = h4_element.text.strip()\n",
    "                                features.append(feature_text)\n",
    "                        product_tags_div = soup.find('div', class_='product-tags')\n",
    "                        if product_tags_div:\n",
    "                            h5_elements = product_tags_div.find_all('h5')\n",
    "                            tags = [h5.text.strip() for h5 in h5_elements]\n",
    "                        else:\n",
    "                            tags = []\n",
    "                        book_id = None\n",
    "                        shabak = None\n",
    "                        page_count = None\n",
    "                        solar_publish_year = None\n",
    "                        gregorian_publish_year = None\n",
    "                        print_series = None\n",
    "                        type_of_print = None\n",
    "                        size = None\n",
    "                        translator_info = {}\n",
    "                        product_info = clearfix.find('table', class_='product-table')\n",
    "                        if product_info:\n",
    "                            attr_elements = product_info.find_all('td')\n",
    "                            for i in range(0, len(attr_elements), 2):\n",
    "                                attr_name = attr_elements[i].text.strip()\n",
    "                                attr_value = attr_elements[i + 1].text.strip()\n",
    "                                if attr_name == 'کد کتاب :':\n",
    "                                    book_id = attr_value\n",
    "                                \n",
    "                                if 'مترجم' in attr_name and attr_value:\n",
    "                                    translator_elements = attr_elements[i + 1].find_all('a', itemprop='author')\n",
    "                                    for translator_element in translator_elements:\n",
    "                                        translator_id = int(translator_element.get('href').split('/profile/')[1].split('-')[0])\n",
    "                                        translator_name = translator_element.find('span', itemprop='name').text.strip()\n",
    "                                        translator_info[translator_name] = translator_id\n",
    "                                if 'شابک' in attr_name:\n",
    "                                    shabak = attr_value\n",
    "                                if 'تعداد صفحه' in attr_name:\n",
    "                                    page_count = attr_value\n",
    "                                if 'سال انتشار شمسی' in attr_name:\n",
    "                                    solar_publish_year = attr_value\n",
    "                                if 'سال انتشار میلادی' in attr_name:\n",
    "                                    gregorian_publish_year = attr_value\n",
    "                                if 'نوع جلد' in attr_name:\n",
    "                                    type_of_print = attr_value\n",
    "                                if 'سری چاپ' in attr_name:\n",
    "                                    print_series = attr_value\n",
    "                                if 'قطع' in attr_name:\n",
    "                                    size = attr_value\n",
    "                        scraped_data = {\n",
    "                            'book_id': book_id,\n",
    "                            'print_series': print_series,\n",
    "                            'size': size,\n",
    "                            'type_of_print': type_of_print,\n",
    "                            'translator': translator_info,\n",
    "                            'shabak': shabak,\n",
    "                            'gregorian_publish_year': gregorian_publish_year,\n",
    "                            'solar_publish_year': solar_publish_year,\n",
    "                            'page_count': page_count,\n",
    "                            'Persian Title': persian_title,\n",
    "                            'English Title': english_title,\n",
    "                            'Off': off_percent,\n",
    "                            'Rate': round(float(data_rating), 2),\n",
    "                            'Break Price': before_price,\n",
    "                            'Special_Price': after_price,\n",
    "                            'Exist': exists_book_text,\n",
    "                            'Publisher': publisher_text,\n",
    "                            'publisher id': publisher_id,\n",
    "                            'Writer': writer_text,\n",
    "                            'writer id': writer_id,\n",
    "                            'Description': description_text,\n",
    "                            'Feature': features,\n",
    "                            'Category': tags\n",
    "                        }\n",
    "                        csv_writer.writerow(scraped_data)\n",
    "                        \n",
    "                    request_count += 1\n",
    "                    if request_count % 500 == 0:\n",
    "                        time.sleep(1)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed for link: {link}\")\n",
    "            print(e)\n",
    "            continue \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BAAZSHOW\\AppData\\Local\\Temp\\ipykernel_10128\\1251195954.py:1: DtypeWarning: Columns (1,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data1 = pd.read_csv('scraped_data4.csv')\n",
      "C:\\Users\\BAAZSHOW\\AppData\\Local\\Temp\\ipykernel_10128\\1251195954.py:2: DtypeWarning: Columns (1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data2 = pd.read_csv('scraped_data5.csv')\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_csv('scraped_data4.csv')\n",
    "data2 = pd.read_csv('scraped_data5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1 = pd.DataFrame(data1)\n",
    "book2 = pd.DataFrame(data2)\n",
    "book = pd.concat([book1, book2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.drop_duplicates(subset=['Persian Title', 'English Title' , 'Off'\t,'Rate'\t,'Break Price',\t'Special_Price'\t,'Exist',\t'Publisher'\t,'Writer'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.to_csv('info_book.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BAAZSHOW\\AppData\\Local\\Temp\\ipykernel_10128\\1131713321.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_cleaning = pd.read_csv('info_book.csv')\n"
     ]
    }
   ],
   "source": [
    "data_cleaning = pd.read_csv('info_book.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning.dropna(subset=['shabak' , 'book_id' , 'publisher id' , 'writer id' , 'Writer'] , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning['translator'].replace(\"{}\", np.nan, inplace=True)\n",
    "data_cleaning['Feature'].replace(\"[]\", np.nan, inplace=True)\n",
    "data_cleaning['translator'].fillna('Unknown',inplace=True)\n",
    "data_cleaning['gregorian_publish_year'].fillna('Unknown',inplace=True)\n",
    "data_cleaning['solar_publish_year'].fillna('Unknown',inplace=True)\n",
    "data_cleaning['page_count'].fillna('Unknown',inplace=True)\n",
    "data_cleaning['English Title'].fillna('Unknown',inplace=True)\n",
    "data_cleaning['Description'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['Feature'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['print_series'].fillna('Unknown',inplace=True)\n",
    "data_cleaning['size'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['type_of_print'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['Break Price'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['Special_Price'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['Off'] = data_cleaning['Off'].str.replace('% تخفیف', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_isbn(isbn):\n",
    "    isbn = ''.join(filter(str.isdigit, str(isbn)))\n",
    "    if len(isbn) == 10:\n",
    "        isbn = '978' + isbn\n",
    "    if len(isbn) == 13:\n",
    "        formatted_isbn = f'{isbn[:3]}-{isbn[3:12]}-{isbn[12]}'\n",
    "        return formatted_isbn\n",
    "    return isbn\n",
    "data_cleaning['shabak'] = data_cleaning['shabak'].apply(fix_isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_solar_year(solar_year):\n",
    "    try:\n",
    "        year_match = re.search(r'\\d+', str(solar_year))\n",
    "        if year_match:\n",
    "            cleaned_year = int(year_match.group())\n",
    "            if 1000 <= cleaned_year <= 1500:  \n",
    "                return cleaned_year\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    return np.nan\n",
    "data_cleaning['solar_publish_year'] = data_cleaning['solar_publish_year'].apply(clean_solar_year)\n",
    "\n",
    "def clean_gregorian_year(gregorian_year):\n",
    "    try:\n",
    "        cleaned_year = re.sub(r'[^\\d-]', '', str(gregorian_year))\n",
    "        \n",
    "        year_match = re.search(r'\\d+', cleaned_year)\n",
    "        if year_match:\n",
    "            cleaned_year = int(year_match.group())\n",
    "            \n",
    "            if 1000 <= cleaned_year <= 3000:\n",
    "                return cleaned_year\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    return np.nan\n",
    "data_cleaning['gregorian_publish_year'] = data_cleaning['gregorian_publish_year'].apply(clean_gregorian_year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning['gregorian_publish_year'] = data_cleaning['gregorian_publish_year'].fillna(0).astype(int)\n",
    "data_cleaning['solar_publish_year'] = data_cleaning['solar_publish_year'].fillna(0).astype(int)\n",
    "data_cleaning['solar_publish_year'] = data_cleaning['solar_publish_year'].replace(0, 'unknown')\n",
    "data_cleaning['gregorian_publish_year'] = data_cleaning['gregorian_publish_year'].replace(0, 'unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(value):\n",
    "    try:\n",
    "        return int(float(value))\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "data_cleaning['print_series'] = data_cleaning['print_series'].apply(convert_to_int)\n",
    "data_cleaning['publisher id'] = data_cleaning['publisher id'].apply(convert_to_int)\n",
    "data_cleaning['writer id'] = data_cleaning['writer id'].apply(convert_to_int)\n",
    "data_cleaning['book_id'] = data_cleaning['book_id'].apply(convert_to_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning['Break Price'] = data_cleaning['Break Price'].str.replace(',', '', regex=True)\n",
    "data_cleaning['Special_Price'] = data_cleaning['Special_Price'].str.replace(',', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>print_series</th>\n",
       "      <th>size</th>\n",
       "      <th>type_of_print</th>\n",
       "      <th>translator</th>\n",
       "      <th>shabak</th>\n",
       "      <th>gregorian_publish_year</th>\n",
       "      <th>solar_publish_year</th>\n",
       "      <th>page_count</th>\n",
       "      <th>Persian Title</th>\n",
       "      <th>...</th>\n",
       "      <th>Break Price</th>\n",
       "      <th>Special_Price</th>\n",
       "      <th>Exist</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>publisher id</th>\n",
       "      <th>Writer</th>\n",
       "      <th>writer id</th>\n",
       "      <th>Description</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71718</td>\n",
       "      <td>80</td>\n",
       "      <td>رقعی</td>\n",
       "      <td>جلد سخت</td>\n",
       "      <td>{'پیمان خاکسار': 164}</td>\n",
       "      <td>978-600229500-2</td>\n",
       "      <td>2008</td>\n",
       "      <td>1402</td>\n",
       "      <td>779</td>\n",
       "      <td>کتاب جزء از کل (مجموعه برگ و نوا)</td>\n",
       "      <td>...</td>\n",
       "      <td>390000</td>\n",
       "      <td>292500</td>\n",
       "      <td>موجود</td>\n",
       "      <td>نشر چشمه</td>\n",
       "      <td>33</td>\n",
       "      <td>استیو تولتز</td>\n",
       "      <td>27</td>\n",
       "      <td>جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...</td>\n",
       "      <td>['برنده جایزه ی NSW Premier سال 2009']</td>\n",
       "      <td>['جایزه ی ان اس دبلیو پرایمر', 'ادبیات استرالی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92408</td>\n",
       "      <td>4</td>\n",
       "      <td>رقعی</td>\n",
       "      <td>جلد سخت</td>\n",
       "      <td>{'گلناز سهرابی': 49301}</td>\n",
       "      <td>978-600989648-6</td>\n",
       "      <td>2008</td>\n",
       "      <td>1401</td>\n",
       "      <td>544</td>\n",
       "      <td>جزء از کل</td>\n",
       "      <td>...</td>\n",
       "      <td>420000</td>\n",
       "      <td>252000</td>\n",
       "      <td>موجود</td>\n",
       "      <td>ندای معاصر (زرین کلک)</td>\n",
       "      <td>1869</td>\n",
       "      <td>استیو تولتز</td>\n",
       "      <td>27</td>\n",
       "      <td>جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...</td>\n",
       "      <td>['برنده جایزه ی NSW Premier سال 2009']</td>\n",
       "      <td>['جایزه ی ان اس دبلیو پرایمر', 'ادبیات استرالی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92404</td>\n",
       "      <td>4</td>\n",
       "      <td>رقعی</td>\n",
       "      <td>شومیز</td>\n",
       "      <td>{'گلناز سهرابی': 49301}</td>\n",
       "      <td>978-600989640-0</td>\n",
       "      <td>2008</td>\n",
       "      <td>1401</td>\n",
       "      <td>544</td>\n",
       "      <td>جزء از کل</td>\n",
       "      <td>...</td>\n",
       "      <td>500000</td>\n",
       "      <td>300000</td>\n",
       "      <td>موجود</td>\n",
       "      <td>ندای معاصر (زرین کلک)</td>\n",
       "      <td>1869</td>\n",
       "      <td>استیو تولتز</td>\n",
       "      <td>27</td>\n",
       "      <td>جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...</td>\n",
       "      <td>['برنده جایزه ی NSW Premier سال 2009']</td>\n",
       "      <td>['جایزه ی ان اس دبلیو پرایمر', 'ادبیات استرالی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>80</td>\n",
       "      <td>رقعی</td>\n",
       "      <td>جلد سخت</td>\n",
       "      <td>{'پیمان خاکسار': 164}</td>\n",
       "      <td>978-600229500-2</td>\n",
       "      <td>2008</td>\n",
       "      <td>1402</td>\n",
       "      <td>656</td>\n",
       "      <td>جزء از کل</td>\n",
       "      <td>...</td>\n",
       "      <td>Not Define</td>\n",
       "      <td>Not Define</td>\n",
       "      <td>ناموجود</td>\n",
       "      <td>نشر چشمه</td>\n",
       "      <td>33</td>\n",
       "      <td>استیو تولتز</td>\n",
       "      <td>27</td>\n",
       "      <td>جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...</td>\n",
       "      <td>['برنده جایزه ی NSW Premier سال 2009']</td>\n",
       "      <td>['جایزه ی ان اس دبلیو پرایمر', 'ادبیات استرالی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>18</td>\n",
       "      <td>رقعی</td>\n",
       "      <td>شومیز</td>\n",
       "      <td>{'مهدی غبرایی': 16}</td>\n",
       "      <td>978-964448297-7</td>\n",
       "      <td>2003</td>\n",
       "      <td>1400</td>\n",
       "      <td>368</td>\n",
       "      <td>کتاب بادبادک باز</td>\n",
       "      <td>...</td>\n",
       "      <td>165000</td>\n",
       "      <td>140250</td>\n",
       "      <td>موجود</td>\n",
       "      <td>نیلوفر</td>\n",
       "      <td>67</td>\n",
       "      <td>خالد حسینی</td>\n",
       "      <td>74</td>\n",
       "      <td>امیر، پسر یک تاجر ثروتمند کابلی و از قبیله ی ص...</td>\n",
       "      <td>['برنده جایزه بوکه سال 2004', 'در سال ۲۰۰۷ میل...</td>\n",
       "      <td>['ادبیات اقتباسی', 'جایزه ی الکس', 'جایزه ی بو...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id  print_series  size type_of_print               translator  \\\n",
       "0    71718            80  رقعی       جلد سخت    {'پیمان خاکسار': 164}   \n",
       "1    92408             4  رقعی       جلد سخت  {'گلناز سهرابی': 49301}   \n",
       "2    92404             4  رقعی         شومیز  {'گلناز سهرابی': 49301}   \n",
       "3       43            80  رقعی       جلد سخت    {'پیمان خاکسار': 164}   \n",
       "4     2030            18  رقعی         شومیز      {'مهدی غبرایی': 16}   \n",
       "\n",
       "            shabak gregorian_publish_year solar_publish_year page_count  \\\n",
       "0  978-600229500-2                   2008               1402        779   \n",
       "1  978-600989648-6                   2008               1401        544   \n",
       "2  978-600989640-0                   2008               1401        544   \n",
       "3  978-600229500-2                   2008               1402        656   \n",
       "4  978-964448297-7                   2003               1400        368   \n",
       "\n",
       "                       Persian Title  ... Break Price Special_Price    Exist  \\\n",
       "0  کتاب جزء از کل (مجموعه برگ و نوا)  ...      390000        292500    موجود   \n",
       "1                          جزء از کل  ...      420000        252000    موجود   \n",
       "2                          جزء از کل  ...      500000        300000    موجود   \n",
       "3                          جزء از کل  ...  Not Define    Not Define  ناموجود   \n",
       "4                   کتاب بادبادک باز  ...      165000        140250    موجود   \n",
       "\n",
       "               Publisher publisher id       Writer writer id  \\\n",
       "0               نشر چشمه           33  استیو تولتز        27   \n",
       "1  ندای معاصر (زرین کلک)         1869  استیو تولتز        27   \n",
       "2  ندای معاصر (زرین کلک)         1869  استیو تولتز        27   \n",
       "3               نشر چشمه           33  استیو تولتز        27   \n",
       "4                 نیلوفر           67   خالد حسینی        74   \n",
       "\n",
       "                                         Description  \\\n",
       "0  جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...   \n",
       "1  جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...   \n",
       "2  جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...   \n",
       "3  جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...   \n",
       "4  امیر، پسر یک تاجر ثروتمند کابلی و از قبیله ی ص...   \n",
       "\n",
       "                                             Feature  \\\n",
       "0             ['برنده جایزه ی NSW Premier سال 2009']   \n",
       "1             ['برنده جایزه ی NSW Premier سال 2009']   \n",
       "2             ['برنده جایزه ی NSW Premier سال 2009']   \n",
       "3             ['برنده جایزه ی NSW Premier سال 2009']   \n",
       "4  ['برنده جایزه بوکه سال 2004', 'در سال ۲۰۰۷ میل...   \n",
       "\n",
       "                                            Category  \n",
       "0  ['جایزه ی ان اس دبلیو پرایمر', 'ادبیات استرالی...  \n",
       "1  ['جایزه ی ان اس دبلیو پرایمر', 'ادبیات استرالی...  \n",
       "2  ['جایزه ی ان اس دبلیو پرایمر', 'ادبیات استرالی...  \n",
       "3  ['جایزه ی ان اس دبلیو پرایمر', 'ادبیات استرالی...  \n",
       "4  ['ادبیات اقتباسی', 'جایزه ی الکس', 'جایزه ی بو...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id                   0\n",
       "print_series              0\n",
       "size                      0\n",
       "type_of_print             0\n",
       "translator                0\n",
       "shabak                    0\n",
       "gregorian_publish_year    0\n",
       "solar_publish_year        0\n",
       "page_count                0\n",
       "Persian Title             0\n",
       "English Title             0\n",
       "Off                       0\n",
       "Rate                      0\n",
       "Break Price               0\n",
       "Special_Price             0\n",
       "Exist                     0\n",
       "Publisher                 0\n",
       "publisher id              0\n",
       "Writer                    0\n",
       "writer id                 0\n",
       "Description               0\n",
       "Feature                   0\n",
       "Category                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaning.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning.to_csv('Book.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BootCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
