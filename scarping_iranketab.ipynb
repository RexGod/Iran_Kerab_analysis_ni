{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://www.iranketab.ir/book'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' response = requests.get(url=main_url , headers=headers)\\nprint(response.status_code) '"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" response = requests.get(url=main_url , headers=headers)\n",
    "print(response.status_code) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" soup = BeautifulSoup(response.content, 'html.parser') \""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" soup = BeautifulSoup(response.content, 'html.parser') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' page_number = 1  # Start with the first page\\n\\nrequest_count = 0\\n\\nsession = requests.Session()\\nvisited_urls = set()\\n\\nwith open(\\'BooksUrls.csv\\', \\'w\\', newline=\\'\\') as csvfile:\\n    url_writer = csv.writer(csvfile)\\n\\n    while True:\\n        current_page_url = f\"{main_url}?pagenumber={page_number}&pagesize=20\"\\n        response = session.get(current_page_url)\\n\\n        if response.status_code != 200:\\n            break\\n\\n        soup = BeautifulSoup(response.text, \\'html.parser\\')\\n        allBooks = soup.find_all(\"a\", class_=\"product-item-link\")\\n        \\n        for book_url in allBooks:\\n            href = book_url.get(\"href\")\\n            if href:\\n                full_url = f\\'{main_url}{href}\\'\\n                if full_url not in visited_urls:\\n                    visited_urls.add(full_url)\\n                    url_writer.writerow([full_url])\\n                    request_count += 1\\n\\n                    if request_count % 500 == 0:\\n                        time.sleep(1)\\n\\n        next_page_link = soup.find(\"a\", {\"data-page-no\": str(page_number + 1)})\\n\\n        if next_page_link:\\n            page_number += 1\\n        else:\\n            break '"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" page_number = 1  # Start with the first page\n",
    "\n",
    "request_count = 0\n",
    "\n",
    "session = requests.Session()\n",
    "visited_urls = set()\n",
    "\n",
    "with open('BooksUrls.csv', 'w', newline='') as csvfile:\n",
    "    url_writer = csv.writer(csvfile)\n",
    "\n",
    "    while True:\n",
    "        current_page_url = f\"{main_url}?pagenumber={page_number}&pagesize=20\"\n",
    "        response = session.get(current_page_url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        allBooks = soup.find_all(\"a\", class_=\"product-item-link\")\n",
    "        \n",
    "        for book_url in allBooks:\n",
    "            href = book_url.get(\"href\")\n",
    "            if href:\n",
    "                full_url = f'{main_url}{href}'\n",
    "                if full_url not in visited_urls:\n",
    "                    visited_urls.add(full_url)\n",
    "                    url_writer.writerow([full_url])\n",
    "                    request_count += 1\n",
    "\n",
    "                    if request_count % 500 == 0:\n",
    "                        time.sleep(1)\n",
    "\n",
    "        next_page_link = soup.find(\"a\", {\"data-page-no\": str(page_number + 1)})\n",
    "\n",
    "        if next_page_link:\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_file = 'final_links.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BookURLs = pd.read_csv(\\'BooksUrls.csv\\', header=None)\\nurls = BookURLs[0].tolist()\\nurls = [url.replace(\"/book\", \"\", 1) for url in urls]\\nlink = pd.DataFrame({\\'final_books_Url\\': urls}) '"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" BookURLs = pd.read_csv('BooksUrls.csv', header=None)\n",
    "urls = BookURLs[0].tolist()\n",
    "urls = [url.replace(\"/book\", \"\", 1) for url in urls]\n",
    "link = pd.DataFrame({'final_books_Url': urls}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link.to_csv('final_books_Url.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "BookURLs = pd.read_csv('final_books_Url.csv', header=None)\n",
    "links = BookURLs[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'scraped_data.csv'\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.DictWriter(file, fieldnames=[\n",
    "        'book_id', 'print_series', 'size', 'type_of_print', 'translator_id', 'translator',\n",
    "        'shabak', 'gregorian_publish_year', 'solar_publish_year', 'page_count',\n",
    "        'Persian Title', 'English Title', 'Off', 'Rate', 'Break Price', 'Special_Price',\n",
    "        'Exist', 'Publisher', 'publisher id', 'Writer', 'writer id', 'Description', 'Feature', 'Category'\n",
    "    ])\n",
    "\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    session = requests.Session()\n",
    "    request_count = 0\n",
    "\n",
    "    for link in links[:10000]:\n",
    "        response = session.get(link)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            clearfix_elements = soup.find_all('div', class_='clearfix')\n",
    "            writer_id = None\n",
    "            publisher_id = None\n",
    "            for clearfix in clearfix_elements:\n",
    "                product_name_element = clearfix.find(class_='product-name')\n",
    "                if product_name_element:\n",
    "                    persian_title = product_name_element.text.strip()\n",
    "\n",
    "                    en_name_element = clearfix.find('div', class_='product-name-englishname ltr')\n",
    "                    if en_name_element:\n",
    "                        english_title = en_name_element.text.strip()\n",
    "                    else:\n",
    "                        english_title = \"\"\n",
    "\n",
    "                    off = clearfix.find('div', style='float: left;font-size: 12px;line-height: 1.375;background-color: #fb3449;color: #fff;padding: 5px 30px 3px;-webkit-border-radius: 0 16px 16px 16px;border-radius: 0 16px 16px 16px;')\n",
    "                    if off:\n",
    "                        off_percent = off.text.strip()\n",
    "                    else:\n",
    "                        off_percent = 0\n",
    "                    rating_div = clearfix.find('div', class_='my-rating')\n",
    "                    if rating_div:\n",
    "                        data_rating = rating_div.get('data-rating')\n",
    "                    else:\n",
    "                        data_rating = 0\n",
    "                    break_price = clearfix.find('span' , class_= 'price price-broken')\n",
    "                    if break_price:\n",
    "                        before_price = break_price.text.strip()\n",
    "                    else :\n",
    "                        before_price = np.nan\n",
    "                    special_price = clearfix.find('span' , class_= 'price price-special')\n",
    "                    if special_price:\n",
    "                        after_price = special_price.text.strip()\n",
    "                    else :\n",
    "                        after_price = np.nan\n",
    "                    exists_book_element = clearfix.find('li', class_='exists-book')\n",
    "                    if exists_book_element:\n",
    "                        exists_book_text = exists_book_element.text.strip()\n",
    "                    else:\n",
    "                        exists_book_text = 'ناموجود'\n",
    "                    publisher_element = clearfix.find('div', class_='col-xs-12 prodoct-attribute-items')\n",
    "                    if publisher_element:\n",
    "                        publisher_span = publisher_element.find('span', class_='prodoct-attribute-item')\n",
    "                        if publisher_span and publisher_span.text.strip() == 'انتشارات:':\n",
    "                            publisher_text = publisher_element.find('span', class_='prodoct-attribute-item').find_next('span').text.strip()\n",
    "                        else:\n",
    "                            publisher_text = \"\"\n",
    "                    else:\n",
    "                        publisher_text = \"\"\n",
    "                    #########################################################################################################################################\n",
    "                    look_for_id = clearfix.find('div', class_='row clearfix')\n",
    "                    for id_publisher_and_writer in look_for_id.find_all('a'):\n",
    "                        if id_publisher_and_writer:\n",
    "                            href = id_publisher_and_writer.get('href')\n",
    "                            if 'publisher' in href:\n",
    "                                publisher_id = int(href.split('/publisher/')[1].split('-')[0])\n",
    "                            elif 'profile' in href:\n",
    "                                writer_id = int(href.split('/profile/')[1].split('-')[0])\n",
    "                    #########################################################################################################################################\n",
    "                    writer_element = clearfix.find('span', itemprop='name')\n",
    "                    if writer_element:\n",
    "                        writer_text = writer_element.text.strip()\n",
    "                    else:\n",
    "                        writer_text = \"\"\n",
    "\n",
    "                    description_element = soup.find('div', class_='product-description')\n",
    "                    if description_element:\n",
    "                        description_text = description_element.text.strip()\n",
    "                    else:\n",
    "                        description_text = \"\"\n",
    "                    feature_elements = soup.find_all('div', class_='product-features')\n",
    "                    features = []\n",
    "                    for feature_element in feature_elements:\n",
    "                        h4_elements = feature_element.find_all('h4')\n",
    "                        for h4_element in h4_elements:\n",
    "                            feature_text = h4_element.text.strip()\n",
    "                            features.append(feature_text)\n",
    "                    product_tags_div = soup.find('div', class_='product-tags')\n",
    "                    if product_tags_div:\n",
    "                        h5_elements = product_tags_div.find_all('h5')\n",
    "                        tags = [h5.text.strip() for h5 in h5_elements]\n",
    "                    else:\n",
    "                        tags = []\n",
    "                    book_id = None\n",
    "                    translator = []\n",
    "                    shabak = None\n",
    "                    page_count = None\n",
    "                    solar_publish_year = None\n",
    "                    gregorian_publish_year = None\n",
    "                    print_series = None\n",
    "                    type_of_print = None\n",
    "                    size = None\n",
    "                    product_info = clearfix.find('table', class_='product-table')\n",
    "                    if product_info:\n",
    "                        attr_elements = product_info.find_all('td')\n",
    "                        for i in range(0, len(attr_elements), 2):\n",
    "                            attr_name = attr_elements[i].text.strip()\n",
    "                            attr_value = attr_elements[i + 1].text.strip()\n",
    "                            if attr_name == 'کد کتاب :':\n",
    "                                book_id = attr_value\n",
    "                            if 'مترجم' in attr_name:\n",
    "                                translator_element = attr_elements[i + 1].find('a', itemprop='author')\n",
    "                                if translator_element:\n",
    "                                    translator_id = int(translator_element.get('href').split('/profile/')[1].split('-')[0])\n",
    "                                    translator.append(attr_value)\n",
    "                                else:\n",
    "                                    translator_id = None\n",
    "                            if 'شابک' in attr_name:\n",
    "                                shabak = attr_value\n",
    "                            if 'تعداد صفحه' in attr_name:\n",
    "                                page_count = attr_value\n",
    "                            if 'سال انتشار شمسی' in attr_name:\n",
    "                                solar_publish_year = attr_value\n",
    "                            if 'سال انتشار میلادی' in attr_name:\n",
    "                                gregorian_publish_year = attr_value\n",
    "                            if 'نوع جلد' in attr_name:\n",
    "                                type_of_print = attr_value\n",
    "                            if 'سری چاپ' in attr_name:\n",
    "                                print_series = attr_value\n",
    "                            if 'قطع' in attr_name:\n",
    "                                size = attr_value\n",
    "                    scraped_data = {\n",
    "                        'book_id': book_id,\n",
    "                        'print_series': print_series,\n",
    "                        'size': size,\n",
    "                        'type_of_print': type_of_print,\n",
    "                        'translator_id': translator_id,\n",
    "                        'translator': translator,\n",
    "                        'shabak': shabak,\n",
    "                        'gregorian_publish_year': gregorian_publish_year,\n",
    "                        'solar_publish_year': solar_publish_year,\n",
    "                        'page_count': page_count,\n",
    "                        'Persian Title': persian_title,\n",
    "                        'English Title': english_title,\n",
    "                        'Off': off_percent,\n",
    "                        'Rate': round(float(data_rating), 2),\n",
    "                        'Break Price': before_price,\n",
    "                        'Special_Price': after_price,\n",
    "                        'Exist': exists_book_text,\n",
    "                        'Publisher': publisher_text,\n",
    "                        'publisher id': publisher_id,\n",
    "                        'Writer': writer_text,\n",
    "                        'writer id': writer_id,\n",
    "                        'Description': description_text,\n",
    "                        'Feature': features,\n",
    "                        'Category': tags\n",
    "                    }\n",
    "                    csv_writer.writerow(scraped_data)\n",
    "                    \n",
    "                request_count += 1\n",
    "                if request_count % 500 == 0:\n",
    "                    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 24 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   book_id                 1 non-null      object \n",
      " 1   print_series            1 non-null      object \n",
      " 2   size                    1 non-null      object \n",
      " 3   type_of_print           1 non-null      object \n",
      " 4   translator_id           1 non-null      int64  \n",
      " 5   translator              1 non-null      object \n",
      " 6   shabak                  1 non-null      object \n",
      " 7   gregorian_publish_year  1 non-null      object \n",
      " 8   solar_publish_year      1 non-null      object \n",
      " 9   page_count              1 non-null      object \n",
      " 10  Persian Title           1 non-null      object \n",
      " 11  English Title           1 non-null      object \n",
      " 12  Off                     1 non-null      object \n",
      " 13  Rate                    1 non-null      float64\n",
      " 14  Break Price             1 non-null      object \n",
      " 15  Special_Price           1 non-null      object \n",
      " 16  Exist                   1 non-null      object \n",
      " 17  Publisher               1 non-null      object \n",
      " 18  publisher id            1 non-null      int64  \n",
      " 19  Writer                  1 non-null      object \n",
      " 20  writer id               1 non-null      int64  \n",
      " 21  Description             1 non-null      object \n",
      " 22  Feature                 1 non-null      object \n",
      " 23  Category                1 non-null      object \n",
      "dtypes: float64(1), int64(3), object(20)\n",
      "memory usage: 320.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "book = pd.DataFrame(scraped_data)\n",
    "book.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.drop_duplicates(subset=['Persian Title', 'English Title' , 'Off'\t,'Rate'\t,'Break Price',\t'Special_Price'\t,'Exist',\t'Publisher'\t,'Writer'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 24 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   book_id                 1 non-null      object \n",
      " 1   print_series            1 non-null      object \n",
      " 2   size                    1 non-null      object \n",
      " 3   type_of_print           1 non-null      object \n",
      " 4   translator_id           1 non-null      int64  \n",
      " 5   translator              1 non-null      object \n",
      " 6   shabak                  1 non-null      object \n",
      " 7   gregorian_publish_year  1 non-null      object \n",
      " 8   solar_publish_year      1 non-null      object \n",
      " 9   page_count              1 non-null      object \n",
      " 10  Persian Title           1 non-null      object \n",
      " 11  English Title           1 non-null      object \n",
      " 12  Off                     1 non-null      object \n",
      " 13  Rate                    1 non-null      float64\n",
      " 14  Break Price             1 non-null      object \n",
      " 15  Special_Price           1 non-null      object \n",
      " 16  Exist                   1 non-null      object \n",
      " 17  Publisher               1 non-null      object \n",
      " 18  publisher id            1 non-null      int64  \n",
      " 19  Writer                  1 non-null      object \n",
      " 20  writer id               1 non-null      int64  \n",
      " 21  Description             1 non-null      object \n",
      " 22  Feature                 1 non-null      object \n",
      " 23  Category                1 non-null      object \n",
      "dtypes: float64(1), int64(3), object(20)\n",
      "memory usage: 320.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "book.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>print_series</th>\n",
       "      <th>size</th>\n",
       "      <th>type_of_print</th>\n",
       "      <th>translator_id</th>\n",
       "      <th>translator</th>\n",
       "      <th>shabak</th>\n",
       "      <th>gregorian_publish_year</th>\n",
       "      <th>solar_publish_year</th>\n",
       "      <th>page_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Break Price</th>\n",
       "      <th>Special_Price</th>\n",
       "      <th>Exist</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>publisher id</th>\n",
       "      <th>Writer</th>\n",
       "      <th>writer id</th>\n",
       "      <th>Description</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>80</td>\n",
       "      <td>رقعی</td>\n",
       "      <td>جلد سخت</td>\n",
       "      <td>164</td>\n",
       "      <td>[پیمان خاکسار]</td>\n",
       "      <td>978-600229-5002</td>\n",
       "      <td>2008</td>\n",
       "      <td>1402</td>\n",
       "      <td>656</td>\n",
       "      <td>...</td>\n",
       "      <td>395,000</td>\n",
       "      <td>375,250</td>\n",
       "      <td>موجود</td>\n",
       "      <td>نشر چشمه</td>\n",
       "      <td>33</td>\n",
       "      <td>استیو تولتز</td>\n",
       "      <td>27</td>\n",
       "      <td>جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...</td>\n",
       "      <td>[برنده جایزه ی NSW Premier سال 2009]</td>\n",
       "      <td>[جایزه ی ان اس دبلیو پرایمر, ادبیات استرالیا, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id print_series  size type_of_print  translator_id      translator  \\\n",
       "0      43           80  رقعی       جلد سخت            164  [پیمان خاکسار]   \n",
       "\n",
       "            shabak gregorian_publish_year solar_publish_year page_count  ...  \\\n",
       "0  978-600229-5002                   2008               1402        656  ...   \n",
       "\n",
       "  Break Price Special_Price  Exist  Publisher publisher id       Writer  \\\n",
       "0     395,000       375,250  موجود   نشر چشمه           33  استیو تولتز   \n",
       "\n",
       "  writer id                                        Description  \\\n",
       "0        27  جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...   \n",
       "\n",
       "                                Feature  \\\n",
       "0  [برنده جایزه ی NSW Premier سال 2009]   \n",
       "\n",
       "                                            Category  \n",
       "0  [جایزه ی ان اس دبلیو پرایمر, ادبیات استرالیا, ...  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.to_csv('info_book.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BootCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
