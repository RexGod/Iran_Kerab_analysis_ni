{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import concurrent.futures\n",
    "                                \n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://www.iranketab.ir/book'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' response = requests.get(url=main_url , headers=headers)\\nprint(response.status_code) '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" response = requests.get(url=main_url , headers=headers)\n",
    "print(response.status_code) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" soup = BeautifulSoup(response.content, 'html.parser') \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" soup = BeautifulSoup(response.content, 'html.parser') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' page_number = 1  # Start with the first page\\n\\nrequest_count = 0\\n\\nsession = requests.Session()\\nvisited_urls = set()\\n\\nwith open(\\'BooksUrls.csv\\', \\'w\\', newline=\\'\\') as csvfile:\\n    url_writer = csv.writer(csvfile)\\n\\n    while True:\\n        current_page_url = f\"{main_url}?pagenumber={page_number}&pagesize=20\"\\n        response = session.get(current_page_url)\\n\\n        if response.status_code != 200:\\n            break\\n\\n        soup = BeautifulSoup(response.text, \\'html.parser\\')\\n        allBooks = soup.find_all(\"a\", class_=\"product-item-link\")\\n        \\n        for book_url in allBooks:\\n            href = book_url.get(\"href\")\\n            if href:\\n                full_url = f\\'{main_url}{href}\\'\\n                if full_url not in visited_urls:\\n                    visited_urls.add(full_url)\\n                    url_writer.writerow([full_url])\\n                    request_count += 1\\n\\n                    if request_count % 500 == 0:\\n                        time.sleep(1)\\n\\n        next_page_link = soup.find(\"a\", {\"data-page-no\": str(page_number + 1)})\\n\\n        if next_page_link:\\n            page_number += 1\\n        else:\\n            break '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" page_number = 1  # Start with the first page\n",
    "\n",
    "request_count = 0\n",
    "\n",
    "session = requests.Session()\n",
    "visited_urls = set()\n",
    "\n",
    "with open('BooksUrls.csv', 'w', newline='') as csvfile:\n",
    "    url_writer = csv.writer(csvfile)\n",
    "\n",
    "    while True:\n",
    "        current_page_url = f\"{main_url}?pagenumber={page_number}&pagesize=20\"\n",
    "        response = session.get(current_page_url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        allBooks = soup.find_all(\"a\", class_=\"product-item-link\")\n",
    "        \n",
    "        for book_url in allBooks:\n",
    "            href = book_url.get(\"href\")\n",
    "            if href:\n",
    "                full_url = f'{main_url}{href}'\n",
    "                if full_url not in visited_urls:\n",
    "                    visited_urls.add(full_url)\n",
    "                    url_writer.writerow([full_url])\n",
    "                    request_count += 1\n",
    "\n",
    "                    if request_count % 500 == 0:\n",
    "                        time.sleep(1)\n",
    "\n",
    "        next_page_link = soup.find(\"a\", {\"data-page-no\": str(page_number + 1)})\n",
    "\n",
    "        if next_page_link:\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_file = 'final_links.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BookURLs = pd.read_csv(\\'BooksUrls.csv\\', header=None)\\nurls = BookURLs[0].tolist()\\nurls = [url.replace(\"/book\", \"\", 1) for url in urls]\\nlink = pd.DataFrame({\\'final_books_Url\\': urls}) '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" BookURLs = pd.read_csv('BooksUrls.csv', header=None)\n",
    "urls = BookURLs[0].tolist()\n",
    "urls = [url.replace(\"/book\", \"\", 1) for url in urls]\n",
    "link = pd.DataFrame({'final_books_Url': urls}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link.to_csv('final_books_Url.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BookURLs = pd.read_csv('final_books_Url.csv', header=None)\n",
    "links = BookURLs[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for link: https://www.iranketab.ir/book/120654-\n",
      "Exceeded 30 redirects.\n"
     ]
    }
   ],
   "source": [
    "retry_strategy = Retry(\n",
    "    total=100,\n",
    "    backoff_factor=2,\n",
    "    status_forcelist=[500, 502, 503, 504],\n",
    ")\n",
    "\n",
    "session = requests.Session()\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "csv_file = 'Book_final.csv'\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.DictWriter(file, fieldnames=[\n",
    "        'book_id', 'print_series', 'size', 'type_of_print', 'translator',\n",
    "        'shabak', 'gregorian_publish_year', 'solar_publish_year', 'page_count',\n",
    "        'Persian Title', 'English Title', 'Off', 'Rate', 'Break Price', 'Special_Price',\n",
    "        'Exist', 'Publisher', 'publisher id', 'Writer', 'writer id', 'Description', 'Feature', 'Category'\n",
    "    ])\n",
    "\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    session = requests.Session()\n",
    "    request_count = 0\n",
    "\n",
    "    for link in links[90000:]:\n",
    "        try:\n",
    "            response = session.get(link)\n",
    "            if response.status_code == 200:\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                clearfix_elements = soup.find_all('div', class_='clearfix')\n",
    "                writer_id = None\n",
    "                publisher_id = None\n",
    "                for clearfix in clearfix_elements:\n",
    "                    product_name_element = clearfix.find(class_='product-name')\n",
    "                    if product_name_element:\n",
    "                        persian_title = product_name_element.text.strip()\n",
    "\n",
    "                        en_name_element = clearfix.find('div', class_='product-name-englishname ltr')\n",
    "                        if en_name_element:\n",
    "                            english_title = en_name_element.text.strip()\n",
    "                        else:\n",
    "                            english_title = \"\"\n",
    "\n",
    "                        off = clearfix.find('div', style='float: left;font-size: 12px;line-height: 1.375;background-color: #fb3449;color: #fff;padding: 5px 30px 3px;-webkit-border-radius: 0 16px 16px 16px;border-radius: 0 16px 16px 16px;')\n",
    "                        if off:\n",
    "                            off_percent = off.text.strip()\n",
    "                        else:\n",
    "                            off_percent = 0\n",
    "                        rating_div = clearfix.find('div', class_='my-rating')\n",
    "                        if rating_div:\n",
    "                            data_rating = rating_div.get('data-rating')\n",
    "                        else:\n",
    "                            data_rating = 0\n",
    "                        break_price = clearfix.find('span' , class_= 'price price-broken')\n",
    "                        if break_price:\n",
    "                            before_price = break_price.text.strip()\n",
    "                        else :\n",
    "                            before_price = np.nan\n",
    "                        special_price = clearfix.find('span' , class_= 'price price-special')\n",
    "                        if special_price:\n",
    "                            after_price = special_price.text.strip()\n",
    "                        else :\n",
    "                            after_price = np.nan\n",
    "                        exists_book_element = clearfix.find('li', class_='exists-book')\n",
    "                        if exists_book_element:\n",
    "                            exists_book_text = exists_book_element.text.strip()\n",
    "                        else:\n",
    "                            exists_book_text = 'ناموجود'\n",
    "                        publisher_element = clearfix.find('div', class_='col-xs-12 prodoct-attribute-items')\n",
    "                        if publisher_element:\n",
    "                            publisher_span = publisher_element.find('span', class_='prodoct-attribute-item')\n",
    "                            if publisher_span and publisher_span.text.strip() == 'انتشارات:':\n",
    "                                publisher_text = publisher_element.find('span', class_='prodoct-attribute-item').find_next('span').text.strip()\n",
    "                            else:\n",
    "                                publisher_text = \"\"\n",
    "                        else:\n",
    "                            publisher_text = \"\"\n",
    "                        #########################################################################################################################################\n",
    "                        look_for_id = clearfix.find('div', class_='row clearfix')\n",
    "                        for id_publisher_and_writer in look_for_id.find_all('a'):\n",
    "                            if id_publisher_and_writer:\n",
    "                                href = id_publisher_and_writer.get('href')\n",
    "                                if 'publisher' in href:\n",
    "                                    publisher_id = int(href.split('/publisher/')[1].split('-')[0])\n",
    "                                elif 'profile' in href:\n",
    "                                    writer_id = int(href.split('/profile/')[1].split('-')[0])\n",
    "                        #########################################################################################################################################\n",
    "                        writer_element = clearfix.find('span', itemprop='name')\n",
    "                        if writer_element:\n",
    "                            writer_text = writer_element.text.strip()\n",
    "                        else:\n",
    "                            writer_text = \"\"\n",
    "\n",
    "                        description_element = soup.find('div', class_='product-description')\n",
    "                        if description_element:\n",
    "                            description_text = description_element.text.strip()\n",
    "                        else:\n",
    "                            description_text = \"\"\n",
    "                        feature_elements = soup.find_all('div', class_='product-features')\n",
    "                        features = []\n",
    "                        for feature_element in feature_elements:\n",
    "                            h4_elements = feature_element.find_all('h4')\n",
    "                            for h4_element in h4_elements:\n",
    "                                feature_text = h4_element.text.strip()\n",
    "                                features.append(feature_text)\n",
    "                        product_tags_div = soup.find('div', class_='product-tags')\n",
    "                        if product_tags_div:\n",
    "                            h5_elements = product_tags_div.find_all('h5')\n",
    "                            tags = [h5.text.strip() for h5 in h5_elements]\n",
    "                        else:\n",
    "                            tags = []\n",
    "                        book_id = None\n",
    "                        shabak = None\n",
    "                        page_count = None\n",
    "                        solar_publish_year = None\n",
    "                        gregorian_publish_year = None\n",
    "                        print_series = None\n",
    "                        type_of_print = None\n",
    "                        size = None\n",
    "                        translator_info = {}\n",
    "                        translator_names = []\n",
    "                        product_info = clearfix.find('table', class_='product-table')\n",
    "                        if product_info:\n",
    "                            attr_elements = product_info.find_all('td')\n",
    "                            for i in range(0, len(attr_elements), 2):\n",
    "                                attr_name = attr_elements[i].text.strip()\n",
    "                                attr_value = attr_elements[i + 1].text.strip()\n",
    "                                if attr_name == 'کد کتاب :':\n",
    "                                    book_id = attr_value\n",
    "\n",
    "                                if 'مترجم' in attr_name and attr_value:\n",
    "                                    translator_elements = attr_elements[i + 1].find_all(itemprop='name')\n",
    "                                    for translator_element in translator_elements:\n",
    "                                        translator_name = translator_element.text.strip()\n",
    "                                        translator_names.append(translator_name)\n",
    "\n",
    "                                if 'شابک' in attr_name:\n",
    "                                    shabak = attr_value\n",
    "                                if 'تعداد صفحه' in attr_name:\n",
    "                                    page_count = attr_value\n",
    "                                if 'سال انتشار شمسی' in attr_name:\n",
    "                                    solar_publish_year = attr_value\n",
    "                                if 'سال انتشار میلادی' in attr_name:\n",
    "                                    gregorian_publish_year = attr_value\n",
    "                                if 'نوع جلد' in attr_name:\n",
    "                                    type_of_print = attr_value\n",
    "                                if 'سری چاپ' in attr_name:\n",
    "                                    print_series = attr_value\n",
    "                                if 'قطع' in attr_name:\n",
    "                                    size = attr_value\n",
    "                        scraped_data = {\n",
    "                            'book_id': book_id,\n",
    "                            'print_series': print_series,\n",
    "                            'size': size,\n",
    "                            'type_of_print': type_of_print,\n",
    "                            'translator': translator_names,\n",
    "                            'shabak': shabak,\n",
    "                            'gregorian_publish_year': gregorian_publish_year,\n",
    "                            'solar_publish_year': solar_publish_year,\n",
    "                            'page_count': page_count,\n",
    "                            'Persian Title': persian_title,\n",
    "                            'English Title': english_title,\n",
    "                            'Off': off_percent,\n",
    "                            'Rate': round(float(data_rating), 2),\n",
    "                            'Break Price': before_price,\n",
    "                            'Special_Price': after_price,\n",
    "                            'Exist': exists_book_text,\n",
    "                            'Publisher': publisher_text,\n",
    "                            'publisher id': publisher_id,\n",
    "                            'Writer': writer_text,\n",
    "                            'writer id': writer_id,\n",
    "                            'Description': description_text,\n",
    "                            'Feature': features,\n",
    "                            'Category': tags\n",
    "                        }\n",
    "                        csv_writer.writerow(scraped_data)\n",
    "                        \n",
    "                    request_count += 1\n",
    "                    if request_count % 500 == 0:\n",
    "                        time.sleep(1)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed for link: {link}\")\n",
    "            print(e)\n",
    "            continue \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Book_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.drop_duplicates(subset=['Persian Title', 'English Title' , 'Off'\t,'Rate'\t,'Break Price',\t'Special_Price'\t,'Exist',\t'Publisher'\t,'Writer'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 110291 entries, 0 to 19868\n",
      "Data columns (total 23 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   book_id                 108054 non-null  float64\n",
      " 1   print_series            106454 non-null  object \n",
      " 2   size                    108124 non-null  object \n",
      " 3   type_of_print           108240 non-null  object \n",
      " 4   translator              110291 non-null  object \n",
      " 5   shabak                  108246 non-null  object \n",
      " 6   gregorian_publish_year  57002 non-null   object \n",
      " 7   solar_publish_year      107208 non-null  object \n",
      " 8   page_count              107697 non-null  object \n",
      " 9   Persian Title           110291 non-null  object \n",
      " 10  English Title           109238 non-null  object \n",
      " 11  Off                     110291 non-null  object \n",
      " 12  Rate                    110291 non-null  float64\n",
      " 13  Break Price             78126 non-null   object \n",
      " 14  Special_Price           78126 non-null   object \n",
      " 15  Exist                   110291 non-null  object \n",
      " 16  Publisher               108054 non-null  object \n",
      " 17  publisher id            110281 non-null  float64\n",
      " 18  Writer                  108062 non-null  object \n",
      " 19  writer id               104296 non-null  float64\n",
      " 20  Description             87163 non-null   object \n",
      " 21  Feature                 110291 non-null  object \n",
      " 22  Category                110291 non-null  object \n",
      "dtypes: float64(4), object(19)\n",
      "memory usage: 20.2+ MB\n"
     ]
    }
   ],
   "source": [
    "book.info(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'book' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Quera\\test\\project1\\Iran_Kerab_analysis_ni\\scarping_iranketab.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Quera/test/project1/Iran_Kerab_analysis_ni/scarping_iranketab.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m book\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39minfo_book_final.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'book' is not defined"
     ]
    }
   ],
   "source": [
    "book.to_csv('info_book_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BAAZSHOW\\AppData\\Local\\Temp\\ipykernel_222216\\3706200143.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_cleaning = pd.read_csv('info_book_final.csv')\n"
     ]
    }
   ],
   "source": [
    "data_cleaning = pd.read_csv('info_book_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning.dropna(subset=['shabak' , 'book_id' , 'publisher id' , 'writer id' , 'Writer'] , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_cleaning.loc[data_cleaning['book_id'] == 100237]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>print_series</th>\n",
       "      <th>size</th>\n",
       "      <th>type_of_print</th>\n",
       "      <th>translator</th>\n",
       "      <th>shabak</th>\n",
       "      <th>gregorian_publish_year</th>\n",
       "      <th>solar_publish_year</th>\n",
       "      <th>page_count</th>\n",
       "      <th>Persian Title</th>\n",
       "      <th>...</th>\n",
       "      <th>Break Price</th>\n",
       "      <th>Special_Price</th>\n",
       "      <th>Exist</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>publisher id</th>\n",
       "      <th>Writer</th>\n",
       "      <th>writer id</th>\n",
       "      <th>Description</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7989</th>\n",
       "      <td>100237</td>\n",
       "      <td>1</td>\n",
       "      <td>رقعی</td>\n",
       "      <td>شومیز</td>\n",
       "      <td>['مینا امیری']</td>\n",
       "      <td>978-622745110-8</td>\n",
       "      <td>2006</td>\n",
       "      <td>1399</td>\n",
       "      <td>207</td>\n",
       "      <td>قانون جذب</td>\n",
       "      <td>...</td>\n",
       "      <td>Not Define</td>\n",
       "      <td>Not Define</td>\n",
       "      <td>ناموجود</td>\n",
       "      <td>آلوس</td>\n",
       "      <td>1827</td>\n",
       "      <td>راندا برن</td>\n",
       "      <td>6970</td>\n",
       "      <td>کتاب راز نوشته راندا برن،ماجراجویی وی در مورد ...</td>\n",
       "      <td>Not Define</td>\n",
       "      <td>['ادبیات استرالیا', 'دهه 2000 میلادی', 'روانشن...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      book_id  print_series  size type_of_print      translator  \\\n",
       "7989   100237             1  رقعی         شومیز  ['مینا امیری']   \n",
       "\n",
       "               shabak gregorian_publish_year solar_publish_year page_count  \\\n",
       "7989  978-622745110-8                   2006               1399        207   \n",
       "\n",
       "     Persian Title  ... Break Price Special_Price    Exist Publisher  \\\n",
       "7989     قانون جذب  ...  Not Define    Not Define  ناموجود      آلوس   \n",
       "\n",
       "     publisher id     Writer writer id  \\\n",
       "7989         1827  راندا برن      6970   \n",
       "\n",
       "                                            Description     Feature  \\\n",
       "7989  کتاب راز نوشته راندا برن،ماجراجویی وی در مورد ...  Not Define   \n",
       "\n",
       "                                               Category  \n",
       "7989  ['ادبیات استرالیا', 'دهه 2000 میلادی', 'روانشن...  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning['Feature'].replace(\"[]\", 'Not Define', inplace=True)\n",
    "data_cleaning['translator'].replace(\"[]\", 'Unknown', inplace=True)\n",
    "data_cleaning['gregorian_publish_year'].fillna('Unknown',inplace=True)\n",
    "data_cleaning['solar_publish_year'].fillna('Unknown',inplace=True)\n",
    "data_cleaning['page_count'].fillna(0,inplace=True)\n",
    "data_cleaning['English Title'].fillna('Unknown',inplace=True)\n",
    "data_cleaning['Description'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['Feature'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['print_series'].fillna('Unknown',inplace=True)\n",
    "data_cleaning['size'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['type_of_print'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['Break Price'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['Special_Price'].fillna('Not Define',inplace=True)\n",
    "data_cleaning['Off'] = data_cleaning['Off'].str.replace('% تخفیف', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_isbn(isbn):\n",
    "    isbn = ''.join(filter(str.isdigit, str(isbn)))\n",
    "    if len(isbn) == 10:\n",
    "        isbn = '978' + isbn\n",
    "    if len(isbn) == 13:\n",
    "        formatted_isbn = f'{isbn[:3]}-{isbn[3:12]}-{isbn[12]}'\n",
    "        return formatted_isbn\n",
    "    return isbn\n",
    "data_cleaning['shabak'] = data_cleaning['shabak'].apply(fix_isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_solar_year(solar_year):\n",
    "    try:\n",
    "        year_match = re.search(r'\\d+', str(solar_year))\n",
    "        if year_match:\n",
    "            cleaned_year = int(year_match.group())\n",
    "            if 1000 <= cleaned_year <= 1500:  \n",
    "                return cleaned_year\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    return np.nan\n",
    "data_cleaning['solar_publish_year'] = data_cleaning['solar_publish_year'].apply(clean_solar_year)\n",
    "\n",
    "def clean_gregorian_year(gregorian_year):\n",
    "    try:\n",
    "        cleaned_year = re.sub(r'[^\\d-]', '', str(gregorian_year))\n",
    "        \n",
    "        year_match = re.search(r'\\d+', cleaned_year)\n",
    "        if year_match:\n",
    "            cleaned_year = int(year_match.group())\n",
    "            \n",
    "            if 1000 <= cleaned_year <= 3000:\n",
    "                return cleaned_year\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    return np.nan\n",
    "data_cleaning['gregorian_publish_year'] = data_cleaning['gregorian_publish_year'].apply(clean_gregorian_year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning['gregorian_publish_year'] = data_cleaning['gregorian_publish_year'].fillna(0).astype(int)\n",
    "data_cleaning['solar_publish_year'] = data_cleaning['solar_publish_year'].fillna(0).astype(int)\n",
    "data_cleaning['solar_publish_year'] = data_cleaning['solar_publish_year'].replace(0, 'unknown')\n",
    "data_cleaning['gregorian_publish_year'] = data_cleaning['gregorian_publish_year'].replace(0, 'unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(value):\n",
    "    try:\n",
    "        return int(float(value))\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "data_cleaning['print_series'] = data_cleaning['print_series'].apply(convert_to_int)\n",
    "data_cleaning['publisher id'] = data_cleaning['publisher id'].apply(convert_to_int)\n",
    "data_cleaning['writer id'] = data_cleaning['writer id'].apply(convert_to_int)\n",
    "data_cleaning['book_id'] = data_cleaning['book_id'].apply(convert_to_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning['Break Price'] = data_cleaning['Break Price'].str.replace(',', '', regex=True)\n",
    "data_cleaning['Special_Price'] = data_cleaning['Special_Price'].str.replace(',', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faghat_digit(text):\n",
    "    if isinstance(text, str):\n",
    "        numeric_part = ''.join(filter(str.isdigit, text))\n",
    "        return numeric_part\n",
    "    elif isinstance(text, (int, float)):\n",
    "        return str(int(text))\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning['page_count'] = data_cleaning['page_count'].apply(faghat_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 103840 entries, 0 to 110290\n",
      "Data columns (total 23 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   book_id                 103840 non-null  int64  \n",
      " 1   print_series            103840 non-null  int64  \n",
      " 2   size                    103840 non-null  object \n",
      " 3   type_of_print           103840 non-null  object \n",
      " 4   translator              103840 non-null  object \n",
      " 5   shabak                  103840 non-null  object \n",
      " 6   gregorian_publish_year  103840 non-null  object \n",
      " 7   solar_publish_year      103840 non-null  object \n",
      " 8   page_count              103840 non-null  object \n",
      " 9   Persian Title           103840 non-null  object \n",
      " 10  English Title           103840 non-null  object \n",
      " 11  Off                     103840 non-null  object \n",
      " 12  Rate                    103840 non-null  float64\n",
      " 13  Break Price             103840 non-null  object \n",
      " 14  Special_Price           103840 non-null  object \n",
      " 15  Exist                   103840 non-null  object \n",
      " 16  Publisher               103840 non-null  object \n",
      " 17  publisher id            103840 non-null  int64  \n",
      " 18  Writer                  103840 non-null  object \n",
      " 19  writer id               103840 non-null  int64  \n",
      " 20  Description             103840 non-null  object \n",
      " 21  Feature                 103840 non-null  object \n",
      " 22  Category                103840 non-null  object \n",
      "dtypes: float64(1), int64(4), object(18)\n",
      "memory usage: 19.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data_cleaning.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id                   0\n",
       "print_series              0\n",
       "size                      0\n",
       "type_of_print             0\n",
       "translator                0\n",
       "shabak                    0\n",
       "gregorian_publish_year    0\n",
       "solar_publish_year        0\n",
       "page_count                0\n",
       "Persian Title             0\n",
       "English Title             0\n",
       "Off                       0\n",
       "Rate                      0\n",
       "Break Price               0\n",
       "Special_Price             0\n",
       "Exist                     0\n",
       "Publisher                 0\n",
       "publisher id              0\n",
       "Writer                    0\n",
       "writer id                 0\n",
       "Description               0\n",
       "Feature                   0\n",
       "Category                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaning.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning.to_csv('Book_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BootCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
