{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://www.iranketab.ir/book'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' response = requests.get(url=main_url , headers=headers)\\nprint(response.status_code) '"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" response = requests.get(url=main_url , headers=headers)\n",
    "print(response.status_code) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" soup = BeautifulSoup(response.content, 'html.parser') \""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" soup = BeautifulSoup(response.content, 'html.parser') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' page_number = 1  # Start with the first page\\n\\nrequest_count = 0\\n\\nsession = requests.Session()\\nvisited_urls = set()\\n\\nwith open(\\'BooksUrls.csv\\', \\'w\\', newline=\\'\\') as csvfile:\\n    url_writer = csv.writer(csvfile)\\n\\n    while True:\\n        current_page_url = f\"{main_url}?pagenumber={page_number}&pagesize=20\"\\n        response = session.get(current_page_url)\\n\\n        if response.status_code != 200:\\n            break\\n\\n        soup = BeautifulSoup(response.text, \\'html.parser\\')\\n        allBooks = soup.find_all(\"a\", class_=\"product-item-link\")\\n        \\n        for book_url in allBooks:\\n            href = book_url.get(\"href\")\\n            if href:\\n                full_url = f\\'{main_url}{href}\\'\\n                if full_url not in visited_urls:\\n                    visited_urls.add(full_url)\\n                    url_writer.writerow([full_url])\\n                    request_count += 1\\n\\n                    if request_count % 500 == 0:\\n                        time.sleep(1)\\n\\n        next_page_link = soup.find(\"a\", {\"data-page-no\": str(page_number + 1)})\\n\\n        if next_page_link:\\n            page_number += 1\\n        else:\\n            break '"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" page_number = 1  # Start with the first page\n",
    "\n",
    "request_count = 0\n",
    "\n",
    "session = requests.Session()\n",
    "visited_urls = set()\n",
    "\n",
    "with open('BooksUrls.csv', 'w', newline='') as csvfile:\n",
    "    url_writer = csv.writer(csvfile)\n",
    "\n",
    "    while True:\n",
    "        current_page_url = f\"{main_url}?pagenumber={page_number}&pagesize=20\"\n",
    "        response = session.get(current_page_url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        allBooks = soup.find_all(\"a\", class_=\"product-item-link\")\n",
    "        \n",
    "        for book_url in allBooks:\n",
    "            href = book_url.get(\"href\")\n",
    "            if href:\n",
    "                full_url = f'{main_url}{href}'\n",
    "                if full_url not in visited_urls:\n",
    "                    visited_urls.add(full_url)\n",
    "                    url_writer.writerow([full_url])\n",
    "                    request_count += 1\n",
    "\n",
    "                    if request_count % 500 == 0:\n",
    "                        time.sleep(1)\n",
    "\n",
    "        next_page_link = soup.find(\"a\", {\"data-page-no\": str(page_number + 1)})\n",
    "\n",
    "        if next_page_link:\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_file = 'final_links.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BookURLs = pd.read_csv(\\'BooksUrls.csv\\', header=None)\\nurls = BookURLs[0].tolist()\\nurls = [url.replace(\"/book\", \"\", 1) for url in urls]\\nlink = pd.DataFrame({\\'final_books_Url\\': urls}) '"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" BookURLs = pd.read_csv('BooksUrls.csv', header=None)\n",
    "urls = BookURLs[0].tolist()\n",
    "urls = [url.replace(\"/book\", \"\", 1) for url in urls]\n",
    "link = pd.DataFrame({'final_books_Url': urls}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link.to_csv('final_books_Url.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "BookURLs = pd.read_csv('final_books_Url.csv', header=None)\n",
    "links = BookURLs[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Quera\\test\\project1\\Iran_Kerab_analysis_ni\\scarping_iranketab.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Quera/test/project1/Iran_Kerab_analysis_ni/scarping_iranketab.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m request_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Quera/test/project1/Iran_Kerab_analysis_ni/scarping_iranketab.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m links[:\u001b[39m10000\u001b[39m]:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Quera/test/project1/Iran_Kerab_analysis_ni/scarping_iranketab.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mget(link)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Quera/test/project1/Iran_Kerab_analysis_ni/scarping_iranketab.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Quera/test/project1/Iran_Kerab_analysis_ni/scarping_iranketab.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(\u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\http\\client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\BAAZSHOW\\anaconda3\\envs\\BootCamp\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_file = 'scraped_data.csv'\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.DictWriter(file, fieldnames=[\n",
    "        'book_id', 'print_series', 'size', 'type_of_print', 'translator_id', 'translator',\n",
    "        'shabak', 'gregorian_publish_year', 'solar_publish_year', 'page_count',\n",
    "        'Persian Title', 'English Title', 'Off', 'Rate', 'Break Price', 'Special_Price',\n",
    "        'Exist', 'Publisher', 'publisher id', 'Writer', 'writer id', 'Description', 'Feature', 'Category'\n",
    "    ])\n",
    "\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    session = requests.Session()\n",
    "    request_count = 0\n",
    "\n",
    "    for link in links[:10000]:\n",
    "        response = session.get(link)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            clearfix_elements = soup.find_all('div', class_='clearfix')\n",
    "            writer_id = None\n",
    "            publisher_id = None\n",
    "            for clearfix in clearfix_elements:\n",
    "                product_name_element = clearfix.find(class_='product-name')\n",
    "                if product_name_element:\n",
    "                    persian_title = product_name_element.text.strip()\n",
    "\n",
    "                    en_name_element = clearfix.find('div', class_='product-name-englishname ltr')\n",
    "                    if en_name_element:\n",
    "                        english_title = en_name_element.text.strip()\n",
    "                    else:\n",
    "                        english_title = \"\"\n",
    "\n",
    "                    off = clearfix.find('div', style='float: left;font-size: 12px;line-height: 1.375;background-color: #fb3449;color: #fff;padding: 5px 30px 3px;-webkit-border-radius: 0 16px 16px 16px;border-radius: 0 16px 16px 16px;')\n",
    "                    if off:\n",
    "                        off_percent = off.text.strip()\n",
    "                    else:\n",
    "                        off_percent = 0\n",
    "                    rating_div = clearfix.find('div', class_='my-rating')\n",
    "                    if rating_div:\n",
    "                        data_rating = rating_div.get('data-rating')\n",
    "                    else:\n",
    "                        data_rating = 0\n",
    "                    break_price = clearfix.find('span' , class_= 'price price-broken')\n",
    "                    if break_price:\n",
    "                        before_price = break_price.text.strip()\n",
    "                    else :\n",
    "                        before_price = np.nan\n",
    "                    special_price = clearfix.find('span' , class_= 'price price-special')\n",
    "                    if special_price:\n",
    "                        after_price = special_price.text.strip()\n",
    "                    else :\n",
    "                        after_price = np.nan\n",
    "                    exists_book_element = clearfix.find('li', class_='exists-book')\n",
    "                    if exists_book_element:\n",
    "                        exists_book_text = exists_book_element.text.strip()\n",
    "                    else:\n",
    "                        exists_book_text = 'ناموجود'\n",
    "                    publisher_element = clearfix.find('div', class_='col-xs-12 prodoct-attribute-items')\n",
    "                    if publisher_element:\n",
    "                        publisher_span = publisher_element.find('span', class_='prodoct-attribute-item')\n",
    "                        if publisher_span and publisher_span.text.strip() == 'انتشارات:':\n",
    "                            publisher_text = publisher_element.find('span', class_='prodoct-attribute-item').find_next('span').text.strip()\n",
    "                        else:\n",
    "                            publisher_text = \"\"\n",
    "                    else:\n",
    "                        publisher_text = \"\"\n",
    "                    #########################################################################################################################################\n",
    "                    look_for_id = clearfix.find('div', class_='row clearfix')\n",
    "                    for id_publisher_and_writer in look_for_id.find_all('a'):\n",
    "                        if id_publisher_and_writer:\n",
    "                            href = id_publisher_and_writer.get('href')\n",
    "                            if 'publisher' in href:\n",
    "                                publisher_id = int(href.split('/publisher/')[1].split('-')[0])\n",
    "                            elif 'profile' in href:\n",
    "                                writer_id = int(href.split('/profile/')[1].split('-')[0])\n",
    "                    #########################################################################################################################################\n",
    "                    writer_element = clearfix.find('span', itemprop='name')\n",
    "                    if writer_element:\n",
    "                        writer_text = writer_element.text.strip()\n",
    "                    else:\n",
    "                        writer_text = \"\"\n",
    "\n",
    "                    description_element = soup.find('div', class_='product-description')\n",
    "                    if description_element:\n",
    "                        description_text = description_element.text.strip()\n",
    "                    else:\n",
    "                        description_text = \"\"\n",
    "                    feature_elements = soup.find_all('div', class_='product-features')\n",
    "                    features = []\n",
    "                    for feature_element in feature_elements:\n",
    "                        h4_elements = feature_element.find_all('h4')\n",
    "                        for h4_element in h4_elements:\n",
    "                            feature_text = h4_element.text.strip()\n",
    "                            features.append(feature_text)\n",
    "                    product_tags_div = soup.find('div', class_='product-tags')\n",
    "                    if product_tags_div:\n",
    "                        h5_elements = product_tags_div.find_all('h5')\n",
    "                        tags = [h5.text.strip() for h5 in h5_elements]\n",
    "                    else:\n",
    "                        tags = []\n",
    "                    book_id = None\n",
    "                    shabak = None\n",
    "                    page_count = None\n",
    "                    solar_publish_year = None\n",
    "                    gregorian_publish_year = None\n",
    "                    print_series = None\n",
    "                    type_of_print = None\n",
    "                    size = None\n",
    "                    translator_info = {}\n",
    "                    product_info = clearfix.find('table', class_='product-table')\n",
    "                    if product_info:\n",
    "                        attr_elements = product_info.find_all('td')\n",
    "                        for i in range(0, len(attr_elements), 2):\n",
    "                            attr_name = attr_elements[i].text.strip()\n",
    "                            attr_value = attr_elements[i + 1].text.strip()\n",
    "                            if attr_name == 'کد کتاب :':\n",
    "                                book_id = attr_value\n",
    "                            if 'مترجم' in attr_name and attr_value:\n",
    "                                translator_elements = attr_elements[i + 1].find_all('a', itemprop='author')\n",
    "                                for translator_element in translator_elements:\n",
    "                                    translator_id = int(translator_element.get('href').split('/profile/')[1].split('-')[0])\n",
    "                                    translator_name = translator_element.find('span', itemprop='name').text.strip()\n",
    "                                    translator_info[translator_id] = translator_name\n",
    "                            if 'شابک' in attr_name:\n",
    "                                shabak = attr_value\n",
    "                            if 'تعداد صفحه' in attr_name:\n",
    "                                page_count = attr_value\n",
    "                            if 'سال انتشار شمسی' in attr_name:\n",
    "                                solar_publish_year = attr_value\n",
    "                            if 'سال انتشار میلادی' in attr_name:\n",
    "                                gregorian_publish_year = attr_value\n",
    "                            if 'نوع جلد' in attr_name:\n",
    "                                type_of_print = attr_value\n",
    "                            if 'سری چاپ' in attr_name:\n",
    "                                print_series = attr_value\n",
    "                            if 'قطع' in attr_name:\n",
    "                                size = attr_value\n",
    "                    scraped_data = {\n",
    "                        'book_id': book_id,\n",
    "                        'print_series': print_series,\n",
    "                        'size': size,\n",
    "                        'type_of_print': type_of_print,\n",
    "                        'translator': translator_info,\n",
    "                        'shabak': shabak,\n",
    "                        'gregorian_publish_year': gregorian_publish_year,\n",
    "                        'solar_publish_year': solar_publish_year,\n",
    "                        'page_count': page_count,\n",
    "                        'Persian Title': persian_title,\n",
    "                        'English Title': english_title,\n",
    "                        'Off': off_percent,\n",
    "                        'Rate': round(float(data_rating), 2),\n",
    "                        'Break Price': before_price,\n",
    "                        'Special_Price': after_price,\n",
    "                        'Exist': exists_book_text,\n",
    "                        'Publisher': publisher_text,\n",
    "                        'publisher id': publisher_id,\n",
    "                        'Writer': writer_text,\n",
    "                        'writer id': writer_id,\n",
    "                        'Description': description_text,\n",
    "                        'Feature': features,\n",
    "                        'Category': tags\n",
    "                    }\n",
    "                    csv_writer.writerow(scraped_data)\n",
    "                    \n",
    "                request_count += 1\n",
    "                if request_count % 500 == 0:\n",
    "                    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 24 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   book_id                 1 non-null      object \n",
      " 1   print_series            1 non-null      object \n",
      " 2   size                    1 non-null      object \n",
      " 3   type_of_print           1 non-null      object \n",
      " 4   translator_id           1 non-null      int64  \n",
      " 5   translator              1 non-null      object \n",
      " 6   shabak                  1 non-null      object \n",
      " 7   gregorian_publish_year  1 non-null      object \n",
      " 8   solar_publish_year      1 non-null      object \n",
      " 9   page_count              1 non-null      object \n",
      " 10  Persian Title           1 non-null      object \n",
      " 11  English Title           1 non-null      object \n",
      " 12  Off                     1 non-null      object \n",
      " 13  Rate                    1 non-null      float64\n",
      " 14  Break Price             1 non-null      object \n",
      " 15  Special_Price           1 non-null      object \n",
      " 16  Exist                   1 non-null      object \n",
      " 17  Publisher               1 non-null      object \n",
      " 18  publisher id            1 non-null      int64  \n",
      " 19  Writer                  1 non-null      object \n",
      " 20  writer id               1 non-null      int64  \n",
      " 21  Description             1 non-null      object \n",
      " 22  Feature                 1 non-null      object \n",
      " 23  Category                1 non-null      object \n",
      "dtypes: float64(1), int64(3), object(20)\n",
      "memory usage: 320.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "book = pd.DataFrame(scraped_data)\n",
    "book.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.drop_duplicates(subset=['Persian Title', 'English Title' , 'Off'\t,'Rate'\t,'Break Price',\t'Special_Price'\t,'Exist',\t'Publisher'\t,'Writer'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 24 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   book_id                 1 non-null      object \n",
      " 1   print_series            1 non-null      object \n",
      " 2   size                    1 non-null      object \n",
      " 3   type_of_print           1 non-null      object \n",
      " 4   translator_id           1 non-null      int64  \n",
      " 5   translator              1 non-null      object \n",
      " 6   shabak                  1 non-null      object \n",
      " 7   gregorian_publish_year  1 non-null      object \n",
      " 8   solar_publish_year      1 non-null      object \n",
      " 9   page_count              1 non-null      object \n",
      " 10  Persian Title           1 non-null      object \n",
      " 11  English Title           1 non-null      object \n",
      " 12  Off                     1 non-null      object \n",
      " 13  Rate                    1 non-null      float64\n",
      " 14  Break Price             1 non-null      object \n",
      " 15  Special_Price           1 non-null      object \n",
      " 16  Exist                   1 non-null      object \n",
      " 17  Publisher               1 non-null      object \n",
      " 18  publisher id            1 non-null      int64  \n",
      " 19  Writer                  1 non-null      object \n",
      " 20  writer id               1 non-null      int64  \n",
      " 21  Description             1 non-null      object \n",
      " 22  Feature                 1 non-null      object \n",
      " 23  Category                1 non-null      object \n",
      "dtypes: float64(1), int64(3), object(20)\n",
      "memory usage: 320.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "book.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>print_series</th>\n",
       "      <th>size</th>\n",
       "      <th>type_of_print</th>\n",
       "      <th>translator_id</th>\n",
       "      <th>translator</th>\n",
       "      <th>shabak</th>\n",
       "      <th>gregorian_publish_year</th>\n",
       "      <th>solar_publish_year</th>\n",
       "      <th>page_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Break Price</th>\n",
       "      <th>Special_Price</th>\n",
       "      <th>Exist</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>publisher id</th>\n",
       "      <th>Writer</th>\n",
       "      <th>writer id</th>\n",
       "      <th>Description</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>80</td>\n",
       "      <td>رقعی</td>\n",
       "      <td>جلد سخت</td>\n",
       "      <td>164</td>\n",
       "      <td>[پیمان خاکسار]</td>\n",
       "      <td>978-600229-5002</td>\n",
       "      <td>2008</td>\n",
       "      <td>1402</td>\n",
       "      <td>656</td>\n",
       "      <td>...</td>\n",
       "      <td>395,000</td>\n",
       "      <td>375,250</td>\n",
       "      <td>موجود</td>\n",
       "      <td>نشر چشمه</td>\n",
       "      <td>33</td>\n",
       "      <td>استیو تولتز</td>\n",
       "      <td>27</td>\n",
       "      <td>جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...</td>\n",
       "      <td>[برنده جایزه ی NSW Premier سال 2009]</td>\n",
       "      <td>[جایزه ی ان اس دبلیو پرایمر, ادبیات استرالیا, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id print_series  size type_of_print  translator_id      translator  \\\n",
       "0      43           80  رقعی       جلد سخت            164  [پیمان خاکسار]   \n",
       "\n",
       "            shabak gregorian_publish_year solar_publish_year page_count  ...  \\\n",
       "0  978-600229-5002                   2008               1402        656  ...   \n",
       "\n",
       "  Break Price Special_Price  Exist  Publisher publisher id       Writer  \\\n",
       "0     395,000       375,250  موجود   نشر چشمه           33  استیو تولتز   \n",
       "\n",
       "  writer id                                        Description  \\\n",
       "0        27  جسپر دین، بیشتر عمرش نمی توانست تصمیم بگیرد چه...   \n",
       "\n",
       "                                Feature  \\\n",
       "0  [برنده جایزه ی NSW Premier سال 2009]   \n",
       "\n",
       "                                            Category  \n",
       "0  [جایزه ی ان اس دبلیو پرایمر, ادبیات استرالیا, ...  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.to_csv('info_book.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BootCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
